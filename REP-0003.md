```
REP: 3
Title: Asynchronous JSON-RPC Server
Author: Radiant Core Contributors <info@radiantfoundation.org>
Status: Draft
Type: Standard
Created: 2025-12-31
License: MIT
```

## Abstract

This REP proposes refactoring the Radiant JSON-RPC server to be fully asynchronous using libevent or C++20 coroutines. The change will improve RPC throughput, enable concurrent request processing, and reduce blocking behavior during intensive operations.

## Specification

### Architecture Overview

The current synchronous RPC server processes one request at a time. The proposed asynchronous architecture will:

1. **Accept connections asynchronously** using libevent
2. **Process requests concurrently** in a thread pool
3. **Handle I/O non-blocking** throughout the request lifecycle
4. **Maintain API compatibility** with existing RPC methods

### Implementation Options

#### Option A: libevent-based (Recommended)

```cpp
class AsyncRPCServer {
private:
    struct event_base* base;
    struct evhttp* http_server;
    std::vector<std::thread> worker_threads;
    ThreadPool thread_pool;
    
public:
    void start();
    void stop();
    void handleRequest(struct evhttp_request* req);
};
```

#### Option B: C++20 Coroutines

```cpp
class AsyncRPCServer {
public:
    std::future<UniValue> processRequest(const JSONRPCRequest& request);
    void handleRequest(struct evhttp_request* req);
    
private:
    std::coroutine_handle<> scheduleTask();
};
```

### Request Processing Flow

```
Client Request → libevent → Request Queue → Worker Thread → RPC Method → Response → Client
```

### Thread Pool Configuration

```ini
# radiant.conf
rpcthreads=8              # Number of RPC worker threads
rpcworkqueue=1000        # Max queued requests
rpctimeout=30            # Request timeout in seconds
```

### API Compatibility

All existing RPC methods maintain identical signatures and behavior:

```cpp
// Existing synchronous method
UniValue getblock(const JSONRPCRequest& request);

// New asynchronous wrapper
std::future<UniValue> getblock_async(const JSONRPCRequest& request);
```

## Motivation

The current synchronous RPC server has several limitations:

1. **Blocking Operations**: Long-running operations block all other RPC calls
2. **Poor Concurrency**: Only one request processed at a time
3. **Timeout Issues**: No mechanism to handle stuck requests
4. **Resource Underutilization**: Multi-core systems not fully utilized
5. **Poor User Experience**: Wallets freeze during intensive operations

Examples of problematic operations:
- `getblock` with large blocks
- `scantxoutset` scanning
- `generatetoaddress` mining
- Database-intensive queries

## Rationale

### Why Asynchronous Architecture?

1. **Proven Technology**: libevent is battle-tested in high-performance servers
2. **Incremental Migration**: Can be adopted gradually without breaking changes
3. **Resource Efficiency**: Better CPU and memory utilization
4. **Scalability**: Handles increased RPC load from growing ecosystem
5. **Future-Proof**: Foundation for more advanced features

### Performance Expectations

- **Concurrent Requests**: 8-16x improvement with 8-core system
- **Response Time**: 50-80% reduction under load
- **Memory Usage**: Slight increase due to threading overhead
- **CPU Usage**: Better distribution across cores

### Alternative Approaches Considered

1. **Multi-process RPC**: Higher memory overhead, complex IPC
2. **Thread-per-request**: Poor scalability, resource exhaustion
3. **Existing synchronous**: Current limitations persist
4. **External RPC proxy**: Adds complexity, single point of failure

libevent-based async provides the best balance of performance and maintainability.

## Backwards Compatibility

This change maintains full backwards compatibility:

- **API Compatibility**: All existing RPC methods work unchanged
- **Configuration**: Existing config options continue to work
- **Behavior**: Response formats and error codes identical
- **Deployment**: Can be enabled/disabled via configuration

### Migration Path

```ini
# Phase 1: Optional async mode
rpcasync=false

# Phase 2: Default enabled
rpcasync=true

# Phase 3: Async only
# (synchronous mode deprecated)
```

## Reference Implementation

### Core Server Implementation

```cpp
// src/rpc/server.h
class AsyncRPCServer {
public:
    AsyncRPCServer();
    ~AsyncRPCServer();
    
    bool start(const std::string& address, int port);
    void stop();
    void waitForShutdown();
    
private:
    struct event_base* eventBase;
    struct evhttp* httpServer;
    ThreadPool workerPool;
    std::atomic<bool> running;
    
    static void requestHandler(struct evhttp_request* req, void* arg);
    void processRequest(struct evhttp_request* req);
    UniValue executeMethod(const std::string& method, const UniValue& params);
};

// src/rpc/server.cpp
void AsyncRPCServer::processRequest(struct evhttp_request* req) {
    // Parse request asynchronously
    auto future = std::async(std::launch::async, [this, req]() {
        return executeRequest(req);
    });
    
    // Handle response when ready
    future.then([this, req](const UniValue& result) {
        sendResponse(req, result);
    });
}
```

### Thread Pool Implementation

```cpp
// src/util/threadpool.h
class ThreadPool {
public:
    ThreadPool(size_t numThreads);
    ~ThreadPool();
    
    template<typename F, typename... Args>
    auto enqueue(F&& f, Args&&... args) 
        -> std::future<typename std::result_of<F(Args...)>::type>;
    
    void shutdown();
    
private:
    std::vector<std::thread> workers;
    std::queue<std::function<void()>> tasks;
    std::mutex queueMutex;
    std::condition_variable condition;
    std::atomic<bool> stop;
};
```

### RPC Method Adaptation

```cpp
// src/rpc/blockchain.cpp
UniValue getblock(const JSONRPCRequest& request) {
    // Existing synchronous implementation
    // ... unchanged ...
}

// New async wrapper (optional)
std::future<UniValue> getblock_async(const JSONRPCRequest& request) {
    return std::async(std::launch::async, [request]() {
        return getblock(request);
    });
}
```

### Configuration Integration

```cpp
// src/init.cpp
void InitializeRPCServer() {
    if (gArgs.GetBoolArg("-rpcasync", false)) {
        rpcServer = std::make_unique<AsyncRPCServer>();
    } else {
        rpcServer = std::make_unique<SyncRPCServer>();
    }
    
    if (!rpcServer->start(rpcBindAddress, rpcPort)) {
        throw std::runtime_error("Failed to start RPC server");
    }
}
```

## Security Considerations

### Potential Security Issues

1. **Resource Exhaustion**: Malicious clients could exhaust thread pool
2. **Request Flooding**: DoS attacks through rapid request submission
3. **Memory Leaks**: Thread-related memory management issues
4. **Race Conditions**: Concurrent access to shared resources

### Mitigation Strategies

1. **Rate Limiting**: Per-client and global request rate limits
2. **Resource Limits**: Configurable thread pool and queue sizes
3. **Request Validation**: Early validation before queuing
4. **Timeout Handling**: Automatic timeout for long-running requests
5. **Memory Management**: RAII and smart pointers throughout

### Rate Limiting Implementation

```cpp
class RateLimiter {
private:
    std::unordered_map<std::string, std::deque<std::chrono::steady_clock::time_point>> clientRequests;
    std::mutex mutex;
    
public:
    bool allowRequest(const std::string& clientIP, int maxRequestsPerMinute);
};
```

### Resource Monitoring

```cpp
class ResourceMonitor {
public:
    struct Metrics {
        size_t activeRequests;
        size_t queuedRequests;
        double avgResponseTime;
        size_t rejectedRequests;
    };
    
    Metrics getMetrics() const;
    bool isOverloaded() const;
};
```

## Testing Strategy

### Unit Tests
- Thread pool functionality
- Request queuing and processing
- Error handling and timeouts
- Resource cleanup

### Integration Tests
- Multiple concurrent requests
- Mixed workload scenarios
- Configuration variations
- Shutdown and restart scenarios

### Performance Tests
- Throughput benchmarks
- Latency measurements
- Resource usage monitoring
- Stress testing with high load

### Compatibility Tests
- All existing RPC methods
- Various client implementations
- Configuration compatibility
- Error response consistency

## Performance Benchmarks

### Expected Improvements

| Metric | Current | Target | Improvement |
|--------|---------|--------|-------------|
| Concurrent Requests | 1 | 16 | 16x |
| Response Time (load) | 2000ms | 400ms | 5x |
| CPU Utilization | 12.5% | 80% | 6.4x |
| Memory Usage | 100MB | 120MB | +20% |

### Test Scenarios

1. **Light Load**: 10 concurrent requests
2. **Medium Load**: 50 concurrent requests  
3. **Heavy Load**: 200 concurrent requests
4. **Mixed Workload**: Various RPC method types
5. **Long Operations**: Database-intensive calls

## Deployment Plan

### Phase 1: Implementation (2-3 weeks)
- Implement async server infrastructure
- Add comprehensive test suite
- Performance benchmarking
- Code review and security audit

### Phase 2: Testing (1-2 weeks)
- Deploy to testnet
- Monitor for stability issues
- Gather performance metrics
- Fix reported issues

### Phase 3: Gradual Rollout (2-3 weeks)
- Optional async mode in mainnet
- Monitor adoption and performance
- Collect user feedback
- Address any issues

### Phase 4: Full Migration (1 week)
- Enable async by default
- Deprecate synchronous mode
- Update documentation
- Monitor production performance

## Configuration Options

```ini
# Asynchronous RPC settings
rpcasync=true                    # Enable async RPC server
rpcthreads=8                     # Number of worker threads
rpcworkqueue=1000               # Max queued requests
rpctimeout=30                   # Request timeout (seconds)
rpcratelimit=100                # Max requests per minute per client
```

## Copyright

This document is licensed under the MIT License.
